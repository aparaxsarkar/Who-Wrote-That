{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75656,
     "status": "ok",
     "timestamp": 1765930202217,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "W_Zd_rlqkCQo",
    "outputId": "cb962cc1-37f6-4119-e8a6-b73b3e38cd5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch datasets scikit-learn -q\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325064,
     "status": "ok",
     "timestamp": 1765930527286,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "N2KwqP3RkOVo",
    "outputId": "e1fa73ca-b07d-4e66-b69f-804b027ee0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "Loading dataset from: /content/drive/MyDrive/train_none.csv\n",
      "Loaded 467985 rows\n",
      "Train size: 171149\n",
      "Test size: 42787\n",
      "\n",
      "Label distribution:\n",
      "model\n",
      "mpt-chat        42787\n",
      "mistral-chat    42787\n",
      "llama-chat      42787\n",
      "gpt4            21394\n",
      "cohere-chat     21394\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classes: ['cohere-chat', 'gpt4', 'llama-chat', 'mistral-chat', 'mpt-chat']\n",
      "Fine-tuning DistilBERT...\n",
      "  Model: distilbert-base-uncased\n",
      "  Max Length: 256\n",
      "  Batch Size: 32\n",
      "  Epochs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing texts in batches...\n",
      "  Tokenized 1000/171149\n",
      "  Tokenized 11000/171149\n",
      "  Tokenized 21000/171149\n",
      "  Tokenized 31000/171149\n",
      "  Tokenized 41000/171149\n",
      "  Tokenized 51000/171149\n",
      "  Tokenized 61000/171149\n",
      "  Tokenized 71000/171149\n",
      "  Tokenized 81000/171149\n",
      "  Tokenized 91000/171149\n",
      "  Tokenized 101000/171149\n",
      "  Tokenized 111000/171149\n",
      "  Tokenized 121000/171149\n",
      "  Tokenized 131000/171149\n",
      "  Tokenized 141000/171149\n",
      "  Tokenized 151000/171149\n",
      "  Tokenized 161000/171149\n",
      "  Tokenized 171000/171149\n",
      "Train tokenization complete!\n",
      "  Tokenized 1000/42787\n",
      "  Tokenized 11000/42787\n",
      "  Tokenized 21000/42787\n",
      "  Tokenized 31000/42787\n",
      "  Tokenized 41000/42787\n",
      "Eval tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "TRAIN_PATH = \"/content/drive/MyDrive/train_none.csv\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive\"\n",
    "\n",
    "MODELS = [\"cohere-chat\", \"gpt4\", \"mistral-chat\", \"mpt-chat\", \"llama-chat\"]\n",
    "RANDOM_STATE = 5\n",
    "\n",
    "# DistilBERT Configuration\n",
    "BERT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "BERT_MAX_LENGTH = 256\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_EPOCHS = 4\n",
    "BERT_LEARNING_RATE = 2e-5\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "print(f\"Loading dataset from: {TRAIN_PATH}\")\n",
    "full_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"Loaded {len(full_df)} rows\")\n",
    "\n",
    "# Split into train/test\n",
    "train_df, test_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=full_df[\"model\"]\n",
    ")\n",
    "\n",
    "# Filter to specific models\n",
    "train_df = train_df[train_df[\"model\"].isin(MODELS)]\n",
    "test_df = test_df[test_df[\"model\"].isin(MODELS)]\n",
    "\n",
    "train_texts = train_df[\"generation\"].astype(str)\n",
    "train_labels = train_df[\"model\"]\n",
    "test_texts = test_df[\"generation\"].astype(str)\n",
    "test_labels = test_df[\"model\"]\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}\")\n",
    "print(f\"Test size: {len(test_texts)}\")\n",
    "print(f\"\\nLabel distribution:\\n{train_labels.value_counts()}\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "\n",
    "print(f\"\\nClasses: {list(le.classes_)}\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "# Save test data for later comparison\n",
    "joblib.dump({\n",
    "    'test_texts': test_texts,\n",
    "    'y_test': y_test,\n",
    "    'test_labels': test_labels,\n",
    "    'test_df': test_df\n",
    "}, os.path.join(MODEL_DIR, \"test_data.pkl\"))\n",
    "\n",
    "\n",
    "# Fine-tune\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bert_model_path = os.path.join(MODEL_DIR, \"distilbert_model\")\n",
    "\n",
    "print(\"Fine-tuning DistilBERT...\")\n",
    "print(f\"  Model: {BERT_MODEL_NAME}\")\n",
    "print(f\"  Max Length: {BERT_MAX_LENGTH}\")\n",
    "print(f\"  Batch Size: {BERT_BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {BERT_EPOCHS}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL_NAME,\n",
    "    num_labels=len(le.classes_)\n",
    ")\n",
    "\n",
    "# Tokenize in batches to avoid RAM crash\n",
    "print(\"\\nTokenizing texts in batches...\")\n",
    "\n",
    "def tokenize_in_batches(texts, labels, tokenizer, max_length, batch_size=1000):\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts.iloc[i:i+batch_size].tolist()\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        all_input_ids.append(encodings['input_ids'])\n",
    "        all_attention_mask.append(encodings['attention_mask'])\n",
    "\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"  Tokenized {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': torch.cat(all_input_ids),\n",
    "        'attention_mask': torch.cat(all_attention_mask),\n",
    "        'labels': labels\n",
    "    })\n",
    "\n",
    "train_dataset = tokenize_in_batches(train_texts, y_train, tokenizer, BERT_MAX_LENGTH)\n",
    "print(\"Train tokenization complete!\")\n",
    "\n",
    "eval_dataset = tokenize_in_batches(test_texts, y_test, tokenizer, BERT_MAX_LENGTH)\n",
    "print(\"Eval tokenization complete!\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=bert_model_path,\n",
    "    num_train_epochs=BERT_EPOCHS,\n",
    "    per_device_train_batch_size=BERT_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BERT_BATCH_SIZE,\n",
    "    learning_rate=BERT_LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 4365624,
     "status": "ok",
     "timestamp": 1765934893340,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "G5ElZU5Jkuwk",
    "outputId": "5e525b5c-3a16-4f54-a522-631aca995701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21396' max='21396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21396/21396 1:11:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.362469</td>\n",
       "      <td>0.862996</td>\n",
       "      <td>0.868745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.328622</td>\n",
       "      <td>0.880852</td>\n",
       "      <td>0.884814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.331171</td>\n",
       "      <td>0.892771</td>\n",
       "      <td>0.896715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.359046</td>\n",
       "      <td>0.897796</td>\n",
       "      <td>0.901741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model saved to: /content/drive/MyDrive/distilbert_model\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "==================================================\n",
      "RESULTS\n",
      "==================================================\n",
      "Accuracy: 0.8978\n",
      "Macro F1: 0.9017\n",
      "\n",
      "✅ Predictions saved to: /content/drive/MyDrive/bert_predictions.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(bert_model_path)\n",
    "tokenizer.save_pretrained(bert_model_path)\n",
    "print(f\"\\n✅ Model saved to: {bert_model_path}\")\n",
    "\n",
    "\n",
    "# Evaluate and save predictions\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "bert_preds = []\n",
    "bert_probs = []\n",
    "\n",
    "for i in range(0, len(test_texts), BERT_BATCH_SIZE):\n",
    "    batch_texts = test_texts.iloc[i:i+BERT_BATCH_SIZE].tolist()\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        batch_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=BERT_MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    bert_preds.extend(preds.cpu().numpy())\n",
    "    bert_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "bert_preds = np.array(bert_preds)\n",
    "bert_probs = np.array(bert_probs)\n",
    "\n",
    "# Save predictions\n",
    "joblib.dump({\n",
    "    'predictions': bert_preds,\n",
    "    'probabilities': bert_probs\n",
    "}, os.path.join(MODEL_DIR, \"bert_predictions.pkl\"))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, bert_preds):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, bert_preds, average='macro'):.4f}\")\n",
    "print(f\"\\n✅ Predictions saved to: {MODEL_DIR}/bert_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "_ElC9P8x8Wc5",
    "outputId": "ed2ae1de-f0c7-44b9-aa69-db797097f08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /content/drive/MyDrive/distilbert_model/4_epochs\n",
      "✅ Model loaded!\n",
      "\n",
      "Tokenizing texts in batches...\n",
      "  Tokenized 1000/171149\n",
      "  Tokenized 21000/171149\n",
      "  Tokenized 41000/171149\n",
      "  Tokenized 61000/171149\n",
      "  Tokenized 81000/171149\n",
      "  Tokenized 101000/171149\n",
      "  Tokenized 121000/171149\n",
      "  Tokenized 141000/171149\n",
      "  Tokenized 161000/171149\n",
      "Train tokenization complete!\n",
      "  Tokenized 1000/42787\n",
      "  Tokenized 21000/42787\n",
      "  Tokenized 41000/42787\n",
      "Eval tokenization complete!\n",
      "\n",
      "Continuing training for 4 more epochs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8444' max='21396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8444/21396 27:02 < 41:29, 5.20 it/s, Epoch 1.58/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.509059</td>\n",
       "      <td>0.882932</td>\n",
       "      <td>0.885780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Continue fine-tuning for 4 more epochs\n",
    "\n",
    "# Paths\n",
    "LOAD_MODEL_PATH = \"/content/drive/MyDrive/distilbert_model/4_epochs\"\n",
    "SAVE_MODEL_PATH = \"/content/drive/MyDrive/distilbert_model/8_epochs\"\n",
    "BERT_MAX_LENGTH = 256\n",
    "BERT_BATCH_SIZE = 32\n",
    "BERT_EPOCHS = 4  # 4 more epochs\n",
    "BERT_LEARNING_RATE = 1e-5  # Slightly lower LR for continued training\n",
    "\n",
    "os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Loading model from: {LOAD_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(LOAD_MODEL_PATH)\n",
    "print(\"✅ Model loaded!\")\n",
    "\n",
    "# Tokenize in batches\n",
    "print(\"\\nTokenizing texts in batches...\")\n",
    "\n",
    "def tokenize_in_batches(texts, labels, tokenizer, max_length, batch_size=1000):\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts.iloc[i:i+batch_size].tolist()\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        all_input_ids.append(encodings['input_ids'])\n",
    "        all_attention_mask.append(encodings['attention_mask'])\n",
    "\n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            print(f\"  Tokenized {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': torch.cat(all_input_ids),\n",
    "        'attention_mask': torch.cat(all_attention_mask),\n",
    "        'labels': labels\n",
    "    })\n",
    "\n",
    "train_dataset = tokenize_in_batches(train_texts, y_train, tokenizer, BERT_MAX_LENGTH)\n",
    "print(\"Train tokenization complete!\")\n",
    "\n",
    "eval_dataset = tokenize_in_batches(test_texts, y_test, tokenizer, BERT_MAX_LENGTH)\n",
    "print(\"Eval tokenization complete!\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_MODEL_PATH,\n",
    "    num_train_epochs=BERT_EPOCHS,\n",
    "    per_device_train_batch_size=BERT_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BERT_BATCH_SIZE,\n",
    "    learning_rate=BERT_LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=200,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"\\nContinuing training for 4 more epochs...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save to new location\n",
    "model.save_pretrained(SAVE_MODEL_PATH)\n",
    "tokenizer.save_pretrained(SAVE_MODEL_PATH)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating...\")\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "bert_preds = []\n",
    "bert_probs = []\n",
    "\n",
    "for i in range(0, len(test_texts), BERT_BATCH_SIZE):\n",
    "    batch_texts = test_texts.iloc[i:i+BERT_BATCH_SIZE].tolist()\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        batch_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=BERT_MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    bert_preds.extend(preds.cpu().numpy())\n",
    "    bert_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "bert_preds = np.array(bert_preds)\n",
    "bert_probs = np.array(bert_probs)\n",
    "\n",
    "# Save predictions\n",
    "joblib.dump({\n",
    "    'predictions': bert_preds,\n",
    "    'probabilities': bert_probs\n",
    "}, os.path.join(SAVE_MODEL_PATH, \"predictions.pkl\"))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTS (8 epochs total)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, bert_preds):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, bert_preds, average='macro'):.4f}\")\n",
    "print(f\"\\n✅ Model saved to: {SAVE_MODEL_PATH}\")\n",
    "print(f\"✅ Original 4-epoch model unchanged at: {LOAD_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aborted training before the 4 epochs were completed because of sharp increase in validation loss on the forst epoch."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPmJ8GNEZTI+9qB3fsVYgcT",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
