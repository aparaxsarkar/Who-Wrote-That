{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DistilBERT Per-Class Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports complete!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Imports complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "BASE_DIR = r\"C:\\Users\\apara\\Desktop\\MDM\\saved_models\"\n",
        "BERT_DIR = r\"C:\\Users\\apara\\Desktop\\MDM\\saved_models\\distilBERT_4_epochs\"\n",
        "LABEL_ENCODER_PATH = f\"{BASE_DIR}\\\\label_encoder.pkl\"\n",
        "FEATURES_PATH = f\"{BASE_DIR}\\\\extracted_features.csv\"\n",
        "TRAIN_CSV_PATH = r\"C:\\Users\\apara\\Desktop\\MDM\\train_none.csv\"\n",
        "\n",
        "MODELS = [\"cohere-chat\", \"gpt4\", \"mistral-chat\", \"mpt-chat\", \"llama-chat\"]\n",
        "RANDOM_STATE = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading label encoder...\n",
            "Classes: ['cohere-chat', 'gpt4', 'llama-chat', 'mistral-chat', 'mpt-chat']\n",
            "\n",
            "Loading extracted features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test samples: 42787\n"
          ]
        }
      ],
      "source": [
        "# Load label encoder\n",
        "print(\"Loading label encoder...\")\n",
        "le = joblib.load(LABEL_ENCODER_PATH)\n",
        "print(f\"Classes: {list(le.classes_)}\")\n",
        "\n",
        "# Load extracted features to get test labels\n",
        "print(\"\\nLoading extracted features...\")\n",
        "features_df = pd.read_csv(FEATURES_PATH)\n",
        "\n",
        "# Get test labels from features file\n",
        "test_labels = features_df[features_df['split'] == 'test']['label']\n",
        "\n",
        "# Encode labels\n",
        "y_test = le.transform(test_labels)\n",
        "print(f\"Test samples: {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading DistilBERT predictions...\n",
            "Predictions loaded: 42787\n"
          ]
        }
      ],
      "source": [
        "# Load DistilBERT predictions\n",
        "print(\"\\nLoading DistilBERT predictions...\")\n",
        "bert_preds = np.load(f\"{BERT_DIR}\\\\bert_preds.npy\")\n",
        "bert_probs = np.load(f\"{BERT_DIR}\\\\bert_probs.npy\")\n",
        "print(f\"Predictions loaded: {len(bert_preds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DISTILBERT (4 EPOCHS) - OVERALL PERFORMANCE\n",
            "============================================================\n",
            "Accuracy: 0.8978\n",
            "Macro F1: 0.9017\n",
            "Weighted F1: 0.8981\n"
          ]
        }
      ],
      "source": [
        "# Overall Performance\n",
        "print(\"=\"*60)\n",
        "print(\"DISTILBERT (4 EPOCHS) - OVERALL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, bert_preds):.4f}\")\n",
        "print(f\"Macro F1: {f1_score(y_test, bert_preds, average='macro'):.4f}\")\n",
        "print(f\"Weighted F1: {f1_score(y_test, bert_preds, average='weighted'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PER-CLASS PERFORMANCE\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " cohere-chat     0.9393    0.8326    0.8827      5348\n",
            "        gpt4     0.9731    0.9274    0.9497      5348\n",
            "  llama-chat     0.9242    0.9492    0.9365     10697\n",
            "mistral-chat     0.8172    0.9050    0.8589     10697\n",
            "    mpt-chat     0.9060    0.8568    0.8807     10697\n",
            "\n",
            "    accuracy                         0.8978     42787\n",
            "   macro avg     0.9120    0.8942    0.9017     42787\n",
            "weighted avg     0.9009    0.8978    0.8981     42787\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Per-Class Performance\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PER-CLASS PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, bert_preds, target_names=le.classes_, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CONFUSION MATRIX\n",
            "============================================================\n",
            "              cohere-chat  gpt4  llama-chat  mistral-chat  mpt-chat\n",
            "cohere-chat          4453    49          90           480       276\n",
            "gpt4                   18  4960         164           137        69\n",
            "llama-chat             19    21       10154           394       109\n",
            "mistral-chat          130    35         354          9681       497\n",
            "mpt-chat              121    32         225          1154      9165\n"
          ]
        }
      ],
      "source": [
        "# Confusion Matrix\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(y_test, bert_preds)\n",
        "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
        "print(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PER-CLASS ACCURACY (RECALL)\n",
            "============================================================\n",
            "  cohere-chat    : 0.8326 (4453/5348)\n",
            "  gpt4           : 0.9274 (4960/5348)\n",
            "  llama-chat     : 0.9492 (10154/10697)\n",
            "  mistral-chat   : 0.9050 (9681/10697)\n",
            "  mpt-chat       : 0.8568 (9165/10697)\n"
          ]
        }
      ],
      "source": [
        "# Per-Class Accuracy (Recall)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PER-CLASS ACCURACY (RECALL)\")\n",
        "print(\"=\"*60)\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    class_mask = (y_test == i)\n",
        "    class_correct = (bert_preds[class_mask] == i).sum()\n",
        "    class_total = class_mask.sum()\n",
        "    class_acc = class_correct / class_total\n",
        "    print(f\"  {class_name:15s}: {class_acc:.4f} ({class_correct}/{class_total})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MOST COMMON MISCLASSIFICATIONS\n",
            "============================================================\n",
            "        True    Predicted  Count     Rate\n",
            "    mpt-chat mistral-chat   1154 0.107881\n",
            "mistral-chat     mpt-chat    497 0.046462\n",
            " cohere-chat mistral-chat    480 0.089753\n",
            "  llama-chat mistral-chat    394 0.036833\n",
            "mistral-chat   llama-chat    354 0.033093\n",
            " cohere-chat     mpt-chat    276 0.051608\n",
            "    mpt-chat   llama-chat    225 0.021034\n",
            "        gpt4   llama-chat    164 0.030666\n",
            "        gpt4 mistral-chat    137 0.025617\n",
            "mistral-chat  cohere-chat    130 0.012153\n"
          ]
        }
      ],
      "source": [
        "# Most Common Errors\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MOST COMMON MISCLASSIFICATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "errors = []\n",
        "for i, true_class in enumerate(le.classes_):\n",
        "    for j, pred_class in enumerate(le.classes_):\n",
        "        if i != j and cm[i, j] > 0:\n",
        "            errors.append({\n",
        "                'True': true_class,\n",
        "                'Predicted': pred_class,\n",
        "                'Count': cm[i, j],\n",
        "                'Rate': cm[i, j] / cm[i].sum()\n",
        "            })\n",
        "\n",
        "errors_df = pd.DataFrame(errors).sort_values('Count', ascending=False).head(10)\n",
        "print(errors_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TABLE FOR REPORT: Per-Class Metrics for DistilBERT\n",
            "============================================================\n",
            "       Model Precision Recall F1-Score  Support\n",
            " cohere-chat      0.94   0.83     0.88     5348\n",
            "        gpt4      0.97   0.93     0.95     5348\n",
            "  llama-chat      0.92   0.95     0.94    10697\n",
            "mistral-chat      0.82   0.91     0.86    10697\n",
            "    mpt-chat      0.91   0.86     0.88    10697\n",
            "   Macro Avg      0.91   0.89     0.90    42787\n",
            "Weighted Avg      0.90   0.90     0.90    42787\n"
          ]
        }
      ],
      "source": [
        "# Table for report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TABLE FOR REPORT: Per-Class Metrics for DistilBERT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "report = classification_report(y_test, bert_preds, target_names=le.classes_, output_dict=True)\n",
        "\n",
        "table_data = []\n",
        "for class_name in le.classes_:\n",
        "    table_data.append({\n",
        "        'Model': class_name,\n",
        "        'Precision': f\"{report[class_name]['precision']:.2f}\",\n",
        "        'Recall': f\"{report[class_name]['recall']:.2f}\",\n",
        "        'F1-Score': f\"{report[class_name]['f1-score']:.2f}\",\n",
        "        'Support': int(report[class_name]['support'])\n",
        "    })\n",
        "\n",
        "# Add macro avg\n",
        "table_data.append({\n",
        "    'Model': 'Macro Avg',\n",
        "    'Precision': f\"{report['macro avg']['precision']:.2f}\",\n",
        "    'Recall': f\"{report['macro avg']['recall']:.2f}\",\n",
        "    'F1-Score': f\"{report['macro avg']['f1-score']:.2f}\",\n",
        "    'Support': int(report['macro avg']['support'])\n",
        "})\n",
        "\n",
        "# Add weighted avg\n",
        "table_data.append({\n",
        "    'Model': 'Weighted Avg',\n",
        "    'Precision': f\"{report['weighted avg']['precision']:.2f}\",\n",
        "    'Recall': f\"{report['weighted avg']['recall']:.2f}\",\n",
        "    'F1-Score': f\"{report['weighted avg']['f1-score']:.2f}\",\n",
        "    'Support': int(report['weighted avg']['support'])\n",
        "})\n",
        "\n",
        "table_df = pd.DataFrame(table_data)\n",
        "print(table_df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
