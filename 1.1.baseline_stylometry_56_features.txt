
DATASET SIZE INFO
Full dataset size: 467985
Train size BEFORE filtering: 374388
Test size BEFORE filtering: 93597

Using models: ['cohere-chat', 'gpt4', 'mistral-chat', 'mpt-chat', 'llama-chat']
Train size AFTER filtering: 171149
Test size AFTER filtering: 42787

FINAL TRAIN label distribution:
model
mpt-chat        42787
llama-chat      42787
mistral-chat    42787
gpt4            21394
cohere-chat     21394
Name: count, dtype: int64

FINAL TEST label distribution:
model
mistral-chat    10697
mpt-chat        10697
llama-chat      10697
cohere-chat      5348
gpt4             5348
Name: count, dtype: int64

Building feature matrices
Feature count: 56
X_train shape: (171149, 56)
X_test shape: (42787, 56)

Classes:
['cohere-chat', 'gpt4', 'llama-chat', 'mistral-chat', 'mpt-chat']

Training RandomForest...

RandomForest Accuracy: 0.7411830696239512
RandomForest Macro F1: 0.735371204220254
RandomForest Confusion Matrix:
 [[3043  124  381 1153  647]
 [  47 4000  820  381  100]
 [  76  166 9431  941   83]
 [ 616  220 1461 7169 1231]
 [ 416  157  338 1716 8070]]

Training XGBoost...

XGB Accuracy: 0.762124009629093
XGB Macro F1: 0.7596766004930948
XGB Confusion Matrix:
 [[3361  129  271  945  642]
 [  67 4366  526  269  120]
 [ 107  235 9334  908  113]
 [ 643  254 1242 7220 1338]
 [ 434  137  274 1524 8328]]

Training LightGBM...
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015217 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 13533
[LightGBM] [Info] Number of data points in the train set: 171149, number of used features: 56
[LightGBM] [Info] Start training from score -2.079424
[LightGBM] [Info] Start training from score -2.079424
[LightGBM] [Info] Start training from score -1.386300
[LightGBM] [Info] Start training from score -1.386300
[LightGBM] [Info] Start training from score -1.386300

LGBM Accuracy: 0.7647649987145628
LGBM Macro F1: 0.7632630580039955
LGBM Confusion Matrix:
 [[3415  124  246  932  631]
 [  62 4430  461  276  119]
 [ 108  238 9349  901  101]
 [ 643  240 1229 7220 1365]
 [ 463  139  253 1534 8308]]

Error analysis (XGB):
Number of errors: 10178

Top confusions:
true_model    pred_model    domain
mpt-chat      mistral-chat  news      358
                            poetry    317
                            wiki      306
mistral-chat  llama-chat    wiki      263
              mpt-chat      reddit    262
                            news      248
              llama-chat    books     240
gpt4          llama-chat    reddit    225
mistral-chat  mpt-chat      books     211
              llama-chat    news      207
Name: count, dtype: int64
