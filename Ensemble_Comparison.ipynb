{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Model Comparison\n",
        "Combines predictions from:\n",
        "- RandomForest\n",
        "- XGBoost\n",
        "- LightGBM\n",
        "- DistilBERT\n",
        "\n",
        "Ensemble methods:\n",
        "- Hard Voting (Majority)\n",
        "- Soft Voting (Average Probabilities)\n",
        "- Weighted Voting\n",
        "- Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports complete!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import torch\n",
        "from scipy.sparse import load_npz, hstack, csr_matrix\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report,\n",
        "    confusion_matrix, cohen_kappa_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "print(\"Imports complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration set!\n",
            "Base directory: C:\\Users\\apara\\Desktop\\MDM\\saved_models\n",
            "BERT directory: C:\\Users\\apara\\Desktop\\MDM\\saved_models\\BERT_4_epochs\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BASE_DIR = r\"C:\\Users\\apara\\Desktop\\MDM\\saved_models\"\n",
        "BERT_DIR = r\"C:\\Users\\apara\\Desktop\\MDM\\saved_models\\BERT_4_epochs\"\n",
        "\n",
        "# Model paths\n",
        "MODEL_PATHS = {\n",
        "    'RandomForest': os.path.join(BASE_DIR, 'random_forest.pkl'),\n",
        "    'XGBoost': os.path.join(BASE_DIR, 'xgboost.pkl'),\n",
        "    'LightGBM': os.path.join(BASE_DIR, 'lightgbm.pkl'),\n",
        "}\n",
        "\n",
        "# Other files\n",
        "LABEL_ENCODER_PATH = os.path.join(BASE_DIR, 'label_encoder.pkl')\n",
        "TFIDF_PATH = os.path.join(BASE_DIR, 'tfidf_vectorizer.pkl')\n",
        "X_TEST_TFIDF_PATH = os.path.join(BASE_DIR, 'X_test_tfidf.npz')\n",
        "FEATURES_PATH = os.path.join(BASE_DIR, 'extracted_features.csv')\n",
        "\n",
        "BERT_MAX_LENGTH = 256\n",
        "BERT_BATCH_SIZE = 32\n",
        "\n",
        "print(\"Configuration set!\")\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"BERT directory: {BERT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading label encoder...\n",
            "Classes: ['cohere-chat', 'gpt4', 'llama-chat', 'mistral-chat', 'mpt-chat']\n",
            "\n",
            "Loading extracted features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test samples: 42787\n",
            "\n",
            "Loading TF-IDF features...\n",
            "Combined test features shape: (42787, 5041)\n",
            "Test texts loaded: 42787\n"
          ]
        }
      ],
      "source": [
        "# Load label encoder and features\n",
        "print(\"Loading label encoder...\")\n",
        "le = joblib.load(LABEL_ENCODER_PATH)\n",
        "print(f\"Classes: {list(le.classes_)}\")\n",
        "\n",
        "# Load extracted features\n",
        "print(\"\\nLoading extracted features...\")\n",
        "features_df = pd.read_csv(FEATURES_PATH)\n",
        "\n",
        "# Split into train/test based on 'split' column\n",
        "test_features = features_df[features_df['split'] == 'test'].drop(['label', 'split'], axis=1)\n",
        "test_labels = features_df[features_df['split'] == 'test']['label']\n",
        "\n",
        "# Encode labels\n",
        "y_test = le.transform(test_labels)\n",
        "print(f\"Test samples: {len(y_test)}\")\n",
        "\n",
        "# Load TF-IDF\n",
        "print(\"\\nLoading TF-IDF features...\")\n",
        "X_test_tfidf = load_npz(X_TEST_TFIDF_PATH)\n",
        "\n",
        "# Combine features\n",
        "X_test_num = csr_matrix(test_features.values)\n",
        "X_test = hstack([X_test_tfidf, X_test_num], format='csr')\n",
        "print(f\"Combined test features shape: {X_test.shape}\")\n",
        "\n",
        "# Get test texts for BERT\n",
        "train_df = pd.read_csv(r\"C:\\Users\\apara\\Desktop\\MDM\\train_none.csv\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "MODELS = [\"cohere-chat\", \"gpt4\", \"mistral-chat\", \"mpt-chat\", \"llama-chat\"]\n",
        "_, test_df = train_test_split(train_df, test_size=0.2, random_state=5, stratify=train_df[\"model\"])\n",
        "test_df = test_df[test_df[\"model\"].isin(MODELS)]\n",
        "test_texts = test_df[\"generation\"].astype(str)\n",
        "print(f\"Test texts loaded: {len(test_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ML models and getting predictions...\n",
            "\n",
            "Loading RandomForest...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.7786, F1: 0.7768\n",
            "Loading XGBoost...\n",
            "  Accuracy: 0.8278, F1: 0.8300\n",
            "Loading LightGBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\apara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.8618, F1: 0.8656\n",
            "\n",
            "‚úÖ All ML models loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load all ML models and get predictions\n",
        "print(\"Loading ML models and getting predictions...\\n\")\n",
        "\n",
        "models = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for name, path in MODEL_PATHS.items():\n",
        "    print(f\"Loading {name}...\")\n",
        "    models[name] = joblib.load(path)\n",
        "    predictions[name] = models[name].predict(X_test)\n",
        "    probabilities[name] = models[name].predict_proba(X_test)\n",
        "    \n",
        "    acc = accuracy_score(y_test, predictions[name])\n",
        "    f1 = f1_score(y_test, predictions[name], average='macro')\n",
        "    print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ All ML models loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DistilBERT model...\n",
            "Using device: cpu\n",
            "Getting BERT predictions...\n",
            "  Processed 0/42787\n",
            "  Processed 20000/42787\n",
            "  Processed 40000/42787\n",
            "\n",
            "DistilBERT - Accuracy: 0.8978, F1: 0.9017\n",
            "\n",
            "‚úÖ DistilBERT loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load DistilBERT and get predictions\n",
        "print(\"Loading DistilBERT model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_DIR)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(BERT_DIR)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "bert_model.to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "bert_preds = []\n",
        "bert_probs = []\n",
        "\n",
        "print(\"Getting BERT predictions...\")\n",
        "for i in range(0, len(test_texts), BERT_BATCH_SIZE):\n",
        "    batch_texts = test_texts.iloc[i:i+BERT_BATCH_SIZE].tolist()\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        batch_texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=BERT_MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**encodings)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "    \n",
        "    bert_preds.extend(preds.cpu().numpy())\n",
        "    bert_probs.extend(probs.cpu().numpy())\n",
        "    \n",
        "    if i % 5000 == 0:\n",
        "        print(f\"  Processed {i}/{len(test_texts)}\")\n",
        "\n",
        "predictions['DistilBERT'] = np.array(bert_preds)\n",
        "probabilities['DistilBERT'] = np.array(bert_probs)\n",
        "\n",
        "acc = accuracy_score(y_test, predictions['DistilBERT'])\n",
        "f1 = f1_score(y_test, predictions['DistilBERT'], average='macro')\n",
        "print(f\"\\nDistilBERT - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "print(\"\\n‚úÖ DistilBERT loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "import pandas as pd\n",
        "pd.DataFrame({'bert_preds': bert_preds}).to_csv('bert_preds.csv', index=False)\n",
        "pd.DataFrame({'bert_probs': bert_probs}).to_csv('bert_probs.csv', index=False)\n",
        "\n",
        "# Save to numpy files (faster to load)\n",
        "import numpy as np\n",
        "np.save('bert_preds.npy', bert_preds)\n",
        "np.save('bert_probs.npy', bert_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INDIVIDUAL MODEL PERFORMANCE\n",
            "============================================================\n",
            "       Model  Accuracy  F1 (Macro)  F1 (Weighted)  Cohen Kappa\n",
            "  DistilBERT  0.897773    0.901723       0.898093     0.868727\n",
            "    LightGBM  0.861780    0.865595       0.861500     0.822834\n",
            "     XGBoost  0.827751    0.829966       0.827406     0.778989\n",
            "RandomForest  0.778648    0.776762       0.777052     0.714240\n"
          ]
        }
      ],
      "source": [
        "# Individual Model Performance Summary\n",
        "print(\"=\"*60)\n",
        "print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "for name, preds in predictions.items():\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    f1_macro = f1_score(y_test, preds, average='macro')\n",
        "    f1_weighted = f1_score(y_test, preds, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, preds)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': acc,\n",
        "        'F1 (Macro)': f1_macro,\n",
        "        'F1 (Weighted)': f1_weighted,\n",
        "        'Cohen Kappa': kappa\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PREDICTION CORRELATION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Pearson Correlation Matrix:\n",
            "              RandomForest  XGBoost  LightGBM  DistilBERT\n",
            "RandomForest         1.000    0.824     0.796       0.719\n",
            "XGBoost              0.824    1.000     0.890       0.759\n",
            "LightGBM             0.796    0.890     1.000       0.800\n",
            "DistilBERT           0.719    0.759     0.800       1.000\n",
            "\n",
            "\n",
            "Pairwise Agreement (% same predictions):\n",
            "  RandomForest vs XGBoost: 86.5%\n",
            "  RandomForest vs LightGBM: 84.1%\n",
            "  RandomForest vs DistilBERT: 77.4%\n",
            "  XGBoost vs LightGBM: 91.9%\n",
            "  XGBoost vs DistilBERT: 81.2%\n",
            "  LightGBM vs DistilBERT: 84.1%\n"
          ]
        }
      ],
      "source": [
        "# Correlation Analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION CORRELATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_names = list(predictions.keys())\n",
        "n_models = len(model_names)\n",
        "\n",
        "# Pearson correlation\n",
        "corr_matrix = np.zeros((n_models, n_models))\n",
        "for i, m1 in enumerate(model_names):\n",
        "    for j, m2 in enumerate(model_names):\n",
        "        if i == j:\n",
        "            corr_matrix[i, j] = 1.0\n",
        "        else:\n",
        "            corr_matrix[i, j], _ = pearsonr(predictions[m1], predictions[m2])\n",
        "\n",
        "corr_df = pd.DataFrame(corr_matrix, index=model_names, columns=model_names)\n",
        "print(\"\\nPearson Correlation Matrix:\")\n",
        "print(corr_df.round(3))\n",
        "\n",
        "# Agreement analysis\n",
        "print(\"\\n\\nPairwise Agreement (% same predictions):\")\n",
        "for i, m1 in enumerate(model_names):\n",
        "    for j, m2 in enumerate(model_names):\n",
        "        if i < j:\n",
        "            agreement = np.mean(predictions[m1] == predictions[m2]) * 100\n",
        "            print(f\"  {m1} vs {m2}: {agreement:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ENSEMBLE METHODS\n",
            "============================================================\n",
            "\n",
            "1. HARD VOTING (Majority Vote)\n",
            "   Accuracy: 0.8606, F1: 0.8654\n",
            "\n",
            "2. SOFT VOTING (Average Probabilities)\n",
            "   Accuracy: 0.9008, F1: 0.9047\n",
            "\n",
            "3. WEIGHTED VOTING (by accuracy)\n",
            "   Weights: {'RandomForest': np.float64(0.231), 'XGBoost': np.float64(0.246), 'LightGBM': np.float64(0.256), 'DistilBERT': np.float64(0.267)}\n",
            "   Accuracy: 0.9028, F1: 0.9067\n",
            "\n",
            "4. WEIGHTED VOTING (BERT boosted)\n",
            "   Weights: {'RandomForest': 0.2, 'XGBoost': 0.2, 'LightGBM': 0.2, 'DistilBERT': 0.4}\n",
            "   Accuracy: 0.9117, F1: 0.9155\n"
          ]
        }
      ],
      "source": [
        "# Ensemble Methods\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENSEMBLE METHODS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ensemble_results = []\n",
        "\n",
        "# 1. Hard Voting (Majority Vote)\n",
        "print(\"\\n1. HARD VOTING (Majority Vote)\")\n",
        "all_preds = np.array([predictions[m] for m in model_names])\n",
        "hard_vote_preds = []\n",
        "for i in range(all_preds.shape[1]):\n",
        "    votes = all_preds[:, i]\n",
        "    hard_vote_preds.append(np.bincount(votes.astype(int), minlength=len(le.classes_)).argmax())\n",
        "hard_vote_preds = np.array(hard_vote_preds)\n",
        "\n",
        "acc = accuracy_score(y_test, hard_vote_preds)\n",
        "f1 = f1_score(y_test, hard_vote_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Hard Voting', 'Accuracy': acc, 'F1 (Macro)': f1})\n",
        "\n",
        "# 2. Soft Voting (Average Probabilities)\n",
        "print(\"\\n2. SOFT VOTING (Average Probabilities)\")\n",
        "avg_probs = np.mean([probabilities[m] for m in model_names], axis=0)\n",
        "soft_vote_preds = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, soft_vote_preds)\n",
        "f1 = f1_score(y_test, soft_vote_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Soft Voting', 'Accuracy': acc, 'F1 (Macro)': f1})\n",
        "\n",
        "# 3. Weighted Voting (by individual accuracy)\n",
        "print(\"\\n3. WEIGHTED VOTING (by accuracy)\")\n",
        "weights = np.array([accuracy_score(y_test, predictions[m]) for m in model_names])\n",
        "weights = weights / weights.sum()\n",
        "print(f\"   Weights: {dict(zip(model_names, weights.round(3)))}\")\n",
        "\n",
        "weighted_probs = np.zeros_like(probabilities[model_names[0]])\n",
        "for i, m in enumerate(model_names):\n",
        "    weighted_probs += weights[i] * probabilities[m]\n",
        "weighted_vote_preds = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, weighted_vote_preds)\n",
        "f1 = f1_score(y_test, weighted_vote_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Weighted Voting', 'Accuracy': acc, 'F1 (Macro)': f1})\n",
        "\n",
        "# 4. Weighted Voting (BERT gets higher weight)\n",
        "print(\"\\n4. WEIGHTED VOTING (BERT boosted)\")\n",
        "bert_boost_weights = {'RandomForest': 1, 'XGBoost': 1, 'LightGBM': 1, 'DistilBERT': 2}\n",
        "total = sum(bert_boost_weights.values())\n",
        "bert_boost_weights = {k: v/total for k, v in bert_boost_weights.items()}\n",
        "print(f\"   Weights: {bert_boost_weights}\")\n",
        "\n",
        "boosted_probs = np.zeros_like(probabilities[model_names[0]])\n",
        "for m in model_names:\n",
        "    boosted_probs += bert_boost_weights[m] * probabilities[m]\n",
        "boosted_preds = np.argmax(boosted_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, boosted_preds)\n",
        "f1 = f1_score(y_test, boosted_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'BERT Boosted', 'Accuracy': acc, 'F1 (Macro)': f1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5. STACKING (Meta-learner on probabilities)\n",
            "   Meta-features shape: (42787, 20)\n",
            "   Accuracy: 0.9178, F1: 0.9218\n",
            "\n",
            "6. STACKING (Predictions as features)\n",
            "   Prediction features shape: (42787, 4)\n",
            "   Accuracy: 0.8592, F1: 0.8550\n"
          ]
        }
      ],
      "source": [
        "# Stacking Ensemble\n",
        "print(\"\\n5. STACKING (Meta-learner on probabilities)\")\n",
        "\n",
        "# Create meta-features from all model probabilities\n",
        "meta_features = np.hstack([probabilities[m] for m in model_names])\n",
        "print(f\"   Meta-features shape: {meta_features.shape}\")\n",
        "\n",
        "# Train a simple logistic regression as meta-learner\n",
        "# Using cross-validation to avoid overfitting\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Split meta-features for proper stacking\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
        "stacking_preds = np.zeros(len(y_test))\n",
        "\n",
        "for train_idx, val_idx in skf.split(meta_features, y_test):\n",
        "    X_meta_train = meta_features[train_idx]\n",
        "    y_meta_train = y_test[train_idx]\n",
        "    X_meta_val = meta_features[val_idx]\n",
        "    \n",
        "    meta_clf = LogisticRegression(max_iter=1000, random_state=5)\n",
        "    meta_clf.fit(X_meta_train, y_meta_train)\n",
        "    stacking_preds[val_idx] = meta_clf.predict(X_meta_val)\n",
        "\n",
        "acc = accuracy_score(y_test, stacking_preds)\n",
        "f1 = f1_score(y_test, stacking_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Stacking (LR)', 'Accuracy': acc, 'F1 (Macro)': f1})\n",
        "\n",
        "# Stacking with predictions as features (not probabilities)\n",
        "print(\"\\n6. STACKING (Predictions as features)\")\n",
        "pred_features = np.column_stack([predictions[m] for m in model_names])\n",
        "print(f\"   Prediction features shape: {pred_features.shape}\")\n",
        "\n",
        "stacking_preds2 = np.zeros(len(y_test))\n",
        "for train_idx, val_idx in skf.split(pred_features, y_test):\n",
        "    X_pred_train = pred_features[train_idx]\n",
        "    y_pred_train = y_test[train_idx]\n",
        "    X_pred_val = pred_features[val_idx]\n",
        "    \n",
        "    meta_clf2 = LogisticRegression(max_iter=1000, random_state=5)\n",
        "    meta_clf2.fit(X_pred_train, y_pred_train)\n",
        "    stacking_preds2[val_idx] = meta_clf2.predict(X_pred_val)\n",
        "\n",
        "acc = accuracy_score(y_test, stacking_preds2)\n",
        "f1 = f1_score(y_test, stacking_preds2, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Stacking (Preds)', 'Accuracy': acc, 'F1 (Macro)': f1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "7. TOP-3 MODELS SOFT VOTING\n",
            "   Top 3 models: ['DistilBERT', 'LightGBM', 'XGBoost']\n",
            "   Accuracy: 0.9069, F1: 0.9107\n",
            "\n",
            "8. ML MODELS ONLY (No BERT)\n",
            "   Accuracy: 0.8482, F1: 0.8516\n"
          ]
        }
      ],
      "source": [
        "# Best Models Only Ensemble\n",
        "print(\"\\n7. TOP-3 MODELS SOFT VOTING\")\n",
        "# Get top 3 models by accuracy\n",
        "top3 = results_df.head(3)['Model'].tolist()\n",
        "print(f\"   Top 3 models: {top3}\")\n",
        "\n",
        "top3_probs = np.mean([probabilities[m] for m in top3], axis=0)\n",
        "top3_preds = np.argmax(top3_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, top3_preds)\n",
        "f1 = f1_score(y_test, top3_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'Top-3 Soft Voting', 'Accuracy': acc, 'F1 (Macro)': f1})\n",
        "\n",
        "# ML only (no BERT)\n",
        "print(\"\\n8. ML MODELS ONLY (No BERT)\")\n",
        "ml_models = ['RandomForest', 'XGBoost', 'LightGBM']\n",
        "ml_probs = np.mean([probabilities[m] for m in ml_models], axis=0)\n",
        "ml_preds = np.argmax(ml_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, ml_preds)\n",
        "f1 = f1_score(y_test, ml_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "ensemble_results.append({'Method': 'ML Only Ensemble', 'Accuracy': acc, 'F1 (Macro)': f1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL COMPARISON - ALL METHODS\n",
            "============================================================\n",
            "           Method       Type  Accuracy  F1 (Macro)\n",
            "    Stacking (LR)   Ensemble  0.917802    0.921785\n",
            "     BERT Boosted   Ensemble  0.911702    0.915538\n",
            "Top-3 Soft Voting   Ensemble  0.906911    0.910677\n",
            "  Weighted Voting   Ensemble  0.902844    0.906706\n",
            "      Soft Voting   Ensemble  0.900788    0.904698\n",
            "       DistilBERT Individual  0.897773    0.901723\n",
            "         LightGBM Individual  0.861780    0.865595\n",
            "      Hard Voting   Ensemble  0.860612    0.865370\n",
            " Stacking (Preds)   Ensemble  0.859233    0.854983\n",
            " ML Only Ensemble   Ensemble  0.848202    0.851589\n",
            "          XGBoost Individual  0.827751    0.829966\n",
            "     RandomForest Individual  0.778648    0.776762\n",
            "\n",
            "============================================================\n",
            "üèÜ BEST METHOD: Stacking (LR)\n",
            "   Type: Ensemble\n",
            "   Accuracy: 0.9178 (91.78%)\n",
            "   F1 (Macro): 0.9218\n",
            "============================================================\n",
            "\n",
            "‚úÖ TARGET 90% ACCURACY ACHIEVED!\n"
          ]
        }
      ],
      "source": [
        "# Final Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL COMPARISON - ALL METHODS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine individual and ensemble results\n",
        "all_results = []\n",
        "\n",
        "# Add individual models\n",
        "for _, row in results_df.iterrows():\n",
        "    all_results.append({\n",
        "        'Method': row['Model'],\n",
        "        'Type': 'Individual',\n",
        "        'Accuracy': row['Accuracy'],\n",
        "        'F1 (Macro)': row['F1 (Macro)']\n",
        "    })\n",
        "\n",
        "# Add ensembles\n",
        "for res in ensemble_results:\n",
        "    all_results.append({\n",
        "        'Method': res['Method'],\n",
        "        'Type': 'Ensemble',\n",
        "        'Accuracy': res['Accuracy'],\n",
        "        'F1 (Macro)': res['F1 (Macro)']\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(all_results).sort_values('Accuracy', ascending=False)\n",
        "print(final_df.to_string(index=False))\n",
        "\n",
        "# Highlight best\n",
        "best = final_df.iloc[0]\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üèÜ BEST METHOD: {best['Method']}\")\n",
        "print(f\"   Type: {best['Type']}\")\n",
        "print(f\"   Accuracy: {best['Accuracy']:.4f} ({best['Accuracy']*100:.2f}%)\")\n",
        "print(f\"   F1 (Macro): {best['F1 (Macro)']:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if best['Accuracy'] >= 0.90:\n",
        "    print(\"\\n‚úÖ TARGET 90% ACCURACY ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Gap to 90%: {(0.90 - best['Accuracy'])*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION REPORT - Stacking (LR)\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " cohere-chat       0.92      0.83      0.88      5348\n",
            "        gpt4       0.98      0.95      0.96      5348\n",
            "  llama-chat       0.92      0.96      0.94     10697\n",
            "mistral-chat       0.84      0.88      0.86     10697\n",
            "    mpt-chat       0.90      0.87      0.89     10697\n",
            "\n",
            "    accuracy                           0.90     42787\n",
            "   macro avg       0.91      0.90      0.90     42787\n",
            "weighted avg       0.90      0.90      0.90     42787\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "              cohere-chat  gpt4  llama-chat  mistral-chat  mpt-chat\n",
            "cohere-chat          4455    41         100           441       311\n",
            "gpt4                   18  5102         151            52        25\n",
            "llama-chat             20    25       10270           315        67\n",
            "mistral-chat          162    35         468          9397       635\n",
            "mpt-chat              162    25         194           998      9318\n"
          ]
        }
      ],
      "source": [
        "# Classification Report for Best Method\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"CLASSIFICATION REPORT - {best['Method']}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get predictions for best method\n",
        "if best['Method'] == 'Soft Voting':\n",
        "    best_preds = soft_vote_preds\n",
        "elif best['Method'] == 'Hard Voting':\n",
        "    best_preds = hard_vote_preds\n",
        "elif best['Method'] == 'Weighted Voting':\n",
        "    best_preds = weighted_vote_preds\n",
        "elif best['Method'] == 'BERT Boosted':\n",
        "    best_preds = boosted_preds\n",
        "elif best['Method'] == 'Top-3 Soft Voting':\n",
        "    best_preds = top3_preds\n",
        "elif best['Method'] in predictions:\n",
        "    best_preds = predictions[best['Method']]\n",
        "else:\n",
        "    best_preds = soft_vote_preds  # Default\n",
        "\n",
        "print(classification_report(y_test, best_preds, target_names=le.classes_))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, best_preds)\n",
        "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
        "print(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Results saved to: C:\\Users\\apara\\Desktop\\MDM\\saved_models\\ensemble_results\n",
            "   - model_comparison.csv\n",
            "   - correlation_matrix.csv\n",
            "   - best_ensemble_predictions.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save Results\n",
        "output_dir = os.path.join(BASE_DIR, 'ensemble_results')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save comparison table\n",
        "final_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
        "\n",
        "# Save correlation matrix\n",
        "corr_df.to_csv(os.path.join(output_dir, 'correlation_matrix.csv'))\n",
        "\n",
        "# Save best predictions\n",
        "joblib.dump({\n",
        "    'predictions': best_preds,\n",
        "    'method': best['Method'],\n",
        "    'accuracy': best['Accuracy'],\n",
        "    'f1': best['F1 (Macro)']\n",
        "}, os.path.join(output_dir, 'best_ensemble_predictions.pkl'))\n",
        "\n",
        "print(f\"\\n‚úÖ Results saved to: {output_dir}\")\n",
        "print(\"   - model_comparison.csv\")\n",
        "print(\"   - correlation_matrix.csv\")\n",
        "print(\"   - best_ensemble_predictions.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **WITHOUT RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FULL ENSEMBLE COMPARISON: WITH vs WITHOUT RANDOM FOREST\n",
            "============================================================\n",
            "\n",
            "1. HARD VOTING (no RF)\n",
            "   Accuracy: 0.8704, F1: 0.8736\n",
            "\n",
            "2. SOFT VOTING (no RF)\n",
            "   Accuracy: 0.9069, F1: 0.9107\n",
            "\n",
            "3. WEIGHTED VOTING by accuracy (no RF)\n",
            "   Weights: {'XGBoost': np.float64(0.32), 'LightGBM': np.float64(0.333), 'DistilBERT': np.float64(0.347)}\n",
            "   Accuracy: 0.9080, F1: 0.9117\n",
            "\n",
            "4. BERT BOOSTED (no RF)\n",
            "   Weights: {'XGBoost': 0.25, 'LightGBM': 0.25, 'DistilBERT': 0.5}\n",
            "   Accuracy: 0.9100, F1: 0.9140\n",
            "\n",
            "5. STACKING on probabilities (no RF)\n",
            "   Meta-features shape: (42787, 15)\n",
            "   Accuracy: 0.9174, F1: 0.9214\n",
            "\n",
            "6. STACKING on predictions (no RF)\n",
            "   Prediction features shape: (42787, 3)\n",
            "   Accuracy: 0.8709, F1: 0.8734\n",
            "\n",
            "7. ML ONLY (no RF) - XGBoost + LightGBM\n",
            "   Accuracy: 0.8520, F1: 0.8555\n",
            "\n",
            "============================================================\n",
            "HEAD TO HEAD: WITH RF vs WITHOUT RF\n",
            "============================================================\n",
            "\n",
            "Method               With RF      Without RF   Diff       Winner    \n",
            "----------------------------------------------------------------------\n",
            "Hard Voting          0.8606       0.8704       +0.0098     NO RF ‚úì\n",
            "Soft Voting          0.9008       0.9069       +0.0061     NO RF ‚úì\n",
            "Weighted Voting      0.9028       0.9080       +0.0052     NO RF ‚úì\n",
            "BERT Boosted         0.9117       0.9100       -0.0017     WITH RF ‚úì\n",
            "Stacking (LR)        0.9178       0.9174       -0.0004     WITH RF ‚úì\n",
            "Stacking (Preds)     0.8592       0.8709       +0.0117     NO RF ‚úì\n",
            "ML Only (no RF)      0.8482       0.8520       +0.0038     NO RF ‚úì\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "Without RF wins: 5/7 comparisons\n",
            "\n",
            "Best WITH RF:    0.9178 (Stacking LR)\n",
            "Best WITHOUT RF: 0.9174\n",
            "\n",
            "RECOMMENDATION: KEEP RandomForest\n"
          ]
        }
      ],
      "source": [
        "# COMPARISON - With vs Without RandomForest\n",
        "print(\"=\"*60)\n",
        "print(\"FULL ENSEMBLE COMPARISON: WITH vs WITHOUT RANDOM FOREST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Models without RF\n",
        "models_no_rf = ['XGBoost', 'LightGBM', 'DistilBERT']\n",
        "\n",
        "no_rf_results = []\n",
        "\n",
        "# 1. Hard Voting (no RF)\n",
        "print(\"\\n1. HARD VOTING (no RF)\")\n",
        "no_rf_all_preds = np.array([predictions[m] for m in models_no_rf])\n",
        "no_rf_hard_preds = []\n",
        "for i in range(no_rf_all_preds.shape[1]):\n",
        "    votes = no_rf_all_preds[:, i]\n",
        "    no_rf_hard_preds.append(np.bincount(votes.astype(int), minlength=len(le.classes_)).argmax())\n",
        "no_rf_hard_preds = np.array(no_rf_hard_preds)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_hard_preds)\n",
        "f1 = f1_score(y_test, no_rf_hard_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'Hard Voting', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 2. Soft Voting (no RF)\n",
        "print(\"\\n2. SOFT VOTING (no RF)\")\n",
        "no_rf_probs = np.mean([probabilities[m] for m in models_no_rf], axis=0)\n",
        "no_rf_soft_preds = np.argmax(no_rf_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_soft_preds)\n",
        "f1 = f1_score(y_test, no_rf_soft_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'Soft Voting', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 3. Weighted Voting (no RF)\n",
        "print(\"\\n3. WEIGHTED VOTING by accuracy (no RF)\")\n",
        "no_rf_weights = np.array([accuracy_score(y_test, predictions[m]) for m in models_no_rf])\n",
        "no_rf_weights = no_rf_weights / no_rf_weights.sum()\n",
        "print(f\"   Weights: {dict(zip(models_no_rf, no_rf_weights.round(3)))}\")\n",
        "\n",
        "no_rf_weighted_probs = np.zeros_like(probabilities[models_no_rf[0]])\n",
        "for i, m in enumerate(models_no_rf):\n",
        "    no_rf_weighted_probs += no_rf_weights[i] * probabilities[m]\n",
        "no_rf_weighted_preds = np.argmax(no_rf_weighted_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_weighted_preds)\n",
        "f1 = f1_score(y_test, no_rf_weighted_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'Weighted Voting', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 4. BERT Boosted (no RF)\n",
        "print(\"\\n4. BERT BOOSTED (no RF)\")\n",
        "no_rf_bert_weights = {'XGBoost': 1, 'LightGBM': 1, 'DistilBERT': 2}\n",
        "total = sum(no_rf_bert_weights.values())\n",
        "no_rf_bert_weights = {k: v/total for k, v in no_rf_bert_weights.items()}\n",
        "print(f\"   Weights: {no_rf_bert_weights}\")\n",
        "\n",
        "no_rf_boosted_probs = np.zeros_like(probabilities[models_no_rf[0]])\n",
        "for m in models_no_rf:\n",
        "    no_rf_boosted_probs += no_rf_bert_weights[m] * probabilities[m]\n",
        "no_rf_boosted_preds = np.argmax(no_rf_boosted_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_boosted_preds)\n",
        "f1 = f1_score(y_test, no_rf_boosted_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'BERT Boosted', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 5. Stacking on probabilities (no RF)\n",
        "print(\"\\n5. STACKING on probabilities (no RF)\")\n",
        "no_rf_meta_features = np.hstack([probabilities[m] for m in models_no_rf])\n",
        "print(f\"   Meta-features shape: {no_rf_meta_features.shape}\")\n",
        "\n",
        "no_rf_stacking_preds = np.zeros(len(y_test))\n",
        "for train_idx, val_idx in skf.split(no_rf_meta_features, y_test):\n",
        "    X_meta_train = no_rf_meta_features[train_idx]\n",
        "    y_meta_train = y_test[train_idx]\n",
        "    X_meta_val = no_rf_meta_features[val_idx]\n",
        "    \n",
        "    meta_clf = LogisticRegression(max_iter=1000, random_state=5)\n",
        "    meta_clf.fit(X_meta_train, y_meta_train)\n",
        "    no_rf_stacking_preds[val_idx] = meta_clf.predict(X_meta_val)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_stacking_preds)\n",
        "f1 = f1_score(y_test, no_rf_stacking_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'Stacking (LR)', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 6. Stacking on predictions (no RF)\n",
        "print(\"\\n6. STACKING on predictions (no RF)\")\n",
        "no_rf_pred_features = np.column_stack([predictions[m] for m in models_no_rf])\n",
        "print(f\"   Prediction features shape: {no_rf_pred_features.shape}\")\n",
        "\n",
        "no_rf_stacking_preds2 = np.zeros(len(y_test))\n",
        "for train_idx, val_idx in skf.split(no_rf_pred_features, y_test):\n",
        "    X_pred_train = no_rf_pred_features[train_idx]\n",
        "    y_pred_train = y_test[train_idx]\n",
        "    X_pred_val = no_rf_pred_features[val_idx]\n",
        "    \n",
        "    meta_clf2 = LogisticRegression(max_iter=1000, random_state=5)\n",
        "    meta_clf2.fit(X_pred_train, y_pred_train)\n",
        "    no_rf_stacking_preds2[val_idx] = meta_clf2.predict(X_pred_val)\n",
        "\n",
        "acc = accuracy_score(y_test, no_rf_stacking_preds2)\n",
        "f1 = f1_score(y_test, no_rf_stacking_preds2, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'Stacking (Preds)', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# 7. Top-2 Soft Voting (no RF) - just XGB + LightGBM + BERT is already top 3\n",
        "print(\"\\n7. ML ONLY (no RF) - XGBoost + LightGBM\")\n",
        "ml_no_rf = ['XGBoost', 'LightGBM']\n",
        "ml_no_rf_probs = np.mean([probabilities[m] for m in ml_no_rf], axis=0)\n",
        "ml_no_rf_preds = np.argmax(ml_no_rf_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, ml_no_rf_preds)\n",
        "f1 = f1_score(y_test, ml_no_rf_preds, average='macro')\n",
        "print(f\"   Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "no_rf_results.append({'Method': 'ML Only (no RF)', 'Accuracy': acc, 'F1': f1})\n",
        "\n",
        "# HEAD TO HEAD COMPARISON\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HEAD TO HEAD: WITH RF vs WITHOUT RF\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Original results with RF\n",
        "with_rf_results = {\n",
        "    'Hard Voting': 0.8606,\n",
        "    'Soft Voting': 0.9008,\n",
        "    'Weighted Voting': 0.9028,\n",
        "    'BERT Boosted': 0.9117,\n",
        "    'Stacking (LR)': 0.9178,\n",
        "    'Stacking (Preds)': 0.8592,\n",
        "    'ML Only': 0.8482\n",
        "}\n",
        "\n",
        "print(f\"\\n{'Method':<20} {'With RF':<12} {'Without RF':<12} {'Diff':<10} {'Winner':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for res in no_rf_results:\n",
        "    method = res['Method']\n",
        "    no_rf_acc = res['Accuracy']\n",
        "    \n",
        "    # Match method names\n",
        "    if method == 'ML Only (no RF)':\n",
        "        with_rf_acc = with_rf_results.get('ML Only', 0)\n",
        "    else:\n",
        "        with_rf_acc = with_rf_results.get(method, 0)\n",
        "    \n",
        "    diff = no_rf_acc - with_rf_acc\n",
        "    winner = \"NO RF ‚úì\" if diff > 0 else \"WITH RF ‚úì\" if diff < 0 else \"TIE\"\n",
        "    \n",
        "    print(f\"{method:<20} {with_rf_acc:<12.4f} {no_rf_acc:<12.4f} {diff:+.4f}     {winner}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "no_rf_wins = sum(1 for res in no_rf_results if res['Accuracy'] > with_rf_results.get(res['Method'].replace(' (no RF)', ''), with_rf_results.get('ML Only', 0)))\n",
        "print(f\"Without RF wins: {no_rf_wins}/{len(no_rf_results)} comparisons\")\n",
        "\n",
        "best_with_rf = max(with_rf_results.values())\n",
        "best_no_rf = max(res['Accuracy'] for res in no_rf_results)\n",
        "print(f\"\\nBest WITH RF:    {best_with_rf:.4f} (Stacking LR)\")\n",
        "print(f\"Best WITHOUT RF: {best_no_rf:.4f}\")\n",
        "print(f\"\\nRECOMMENDATION: {'DROP RandomForest' if best_no_rf > best_with_rf else 'KEEP RandomForest'}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
