{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch datasets scikit-learn xgboost lightgbm shap textblob nltk pandas numpy scipy joblib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24376,
     "status": "ok",
     "timestamp": 1765918704024,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "lQPJ1xZZ88G4",
    "outputId": "fd387140-03b5-4622-a4d1-f4f5ca99e9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update the path to your file in Google Drive\n",
    "TRAIN_PATH = '/content/drive/MyDrive/train_none.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1520,
     "status": "ok",
     "timestamp": 1765918981556,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "0nrFddHg2Lto",
    "outputId": "a6ed80a8-4c31-4434-e511-c0089a476448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "Setup complete!\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    classification_report, cohen_kappa_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz, load_npz\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except AttributeError:\n",
    "    # Fallback if NLTK has issues\n",
    "    !python -m nltk.downloader stopwords\n",
    "\n",
    "# Transformers for BERT\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "MODEL_DIR = \"saved_models\"\n",
    "FEATURES_CSV = \"extracted_features.csv\"\n",
    "\n",
    "MAX_TRAIN = 20000\n",
    "TOP_K_MODELS = 10\n",
    "MODELS = [\"cohere-chat\", \"gpt4\", \"mistral-chat\", \"mpt-chat\", \"llama-chat\"]\n",
    "\n",
    "RANDOM_STATE = 5\n",
    "RUN_SHAP = False\n",
    "\n",
    "# BERT Configuration\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "BERT_MAX_LENGTH = 512\n",
    "BERT_BATCH_SIZE = 16\n",
    "BERT_EPOCHS = 3\n",
    "BERT_LEARNING_RATE = 2e-5\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\nSetup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsds9plN8Z4H"
   },
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1765918981595,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "feature_defs",
    "outputId": "e2bebdc0-e91f-4efa-e82d-04f72aa1dc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Feature word sets\n",
    "FUNCTION_WORDS = {\n",
    "    \"the\",\"and\",\"to\",\"of\",\"in\",\"that\",\"is\",\"it\",\"for\",\"on\",\"with\",\n",
    "    \"as\",\"was\",\"at\",\"by\",\"an\",\"be\",\"this\",\"from\",\"or\",\"are\"\n",
    "}\n",
    "\n",
    "PREPOSITIONS = {\n",
    "    \"about\",\"above\",\"across\",\"after\",\"against\",\"along\",\"among\",\"around\",\n",
    "    \"at\",\"before\",\"behind\",\"below\",\"beneath\",\"beside\",\"besides\",\"between\",\n",
    "    \"beyond\",\"but\",\"by\",\"concerning\",\"considering\",\"despite\",\"down\",\"during\",\n",
    "    \"except\",\"for\",\"from\",\"in\",\"inside\",\"into\",\"like\",\"near\",\"of\",\"off\",\n",
    "    \"on\",\"onto\",\"outside\",\"over\",\"past\",\"regarding\",\"round\",\"since\",\"through\",\n",
    "    \"throughout\",\"till\",\"to\",\"toward\",\"towards\",\"under\",\"underneath\",\"until\",\n",
    "    \"up\",\"upon\",\"with\",\"within\",\"without\"\n",
    "}\n",
    "\n",
    "LEFT_WORDS = {\n",
    "    \"climate\",\"equity\",\"union\",\"unions\",\"welfare\",\"redistribution\",\"progressive\",\n",
    "    \"socialism\",\"socialist\",\"labor\",\"labour\",\"feminism\",\"feminist\",\"diversity\",\n",
    "    \"inclusion\",\"green\",\"regulation\",\"regulations\",\"taxing\",\"taxes\",\"public\",\n",
    "    \"universal\",\"healthcare\",\"medicare\",\"woke\"\n",
    "}\n",
    "\n",
    "RIGHT_WORDS = {\n",
    "    \"patriot\",\"patriotism\",\"conservative\",\"conservatism\",\"freedom\",\"liberty\",\n",
    "    \"gun\",\"guns\",\"firearm\",\"border\",\"borders\",\"immigration\",\"immigrant\",\n",
    "    \"military\",\"defense\",\"prolife\",\"pro-life\",\"nationalism\",\"nationalist\",\n",
    "    \"tax-cut\",\"taxcuts\",\"small-government\",\"regulation-free\",\"religious\",\n",
    "    \"traditional\",\"family-values\"\n",
    "}\n",
    "\n",
    "US_SPELLINGS = {\n",
    "    \"color\",\"colors\",\"favorite\",\"center\",\"organize\",\"organizes\",\"organized\",\n",
    "    \"analyze\",\"analyzes\",\"analyzed\",\"defense\",\"meter\",\"liter\",\"theater\"\n",
    "}\n",
    "\n",
    "UK_SPELLINGS = {\n",
    "    \"colour\",\"colours\",\"favourite\",\"centre\",\"organise\",\"organises\",\"organised\",\n",
    "    \"analyse\",\"analyses\",\"analysed\",\"defence\",\"metre\",\"litre\",\"theatre\"\n",
    "}\n",
    "\n",
    "def count_syllables(word: str) -> int:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "    if not word:\n",
    "        return 0\n",
    "    vowels = \"aeiouy\"\n",
    "    groups = re.findall(r'[aeiouy]+', word)\n",
    "    syllables = len(groups)\n",
    "    if word.endswith(\"e\") and syllables > 1:\n",
    "        syllables -= 1\n",
    "    return max(syllables, 1)\n",
    "\n",
    "def extract_lexical_features(text):\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    sentences = [s for s in re.split(r\"[.!?]+\", text) if s.strip()]\n",
    "    num_chars = len(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = max(len(sentences), 1)\n",
    "\n",
    "    word_lengths = [len(w) for w in words] if words else [0]\n",
    "    vocab = set(words)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    counts = Counter(words)\n",
    "    punct = re.findall(r\"[.,;:!?]\", text)\n",
    "\n",
    "    if sentences:\n",
    "        sent_lens = [len(s.split()) for s in sentences]\n",
    "        max_sent_len = max(sent_lens)\n",
    "        min_sent_len = min(sent_lens)\n",
    "    else:\n",
    "        max_sent_len = 0\n",
    "        min_sent_len = 0\n",
    "\n",
    "    return {\n",
    "        \"num_chars\": num_chars,\n",
    "        \"num_words\": num_words,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"avg_word_len\": float(np.mean(word_lengths)),\n",
    "        \"std_word_len\": float(np.std(word_lengths)),\n",
    "        \"min_word_len\": float(np.min(word_lengths)),\n",
    "        \"max_word_len\": float(np.max(word_lengths)),\n",
    "        \"avg_sentence_len\": float(num_words / num_sentences),\n",
    "        \"max_sentence_len\": float(max_sent_len),\n",
    "        \"min_sentence_len\": float(min_sent_len),\n",
    "        \"vocab_size\": float(vocab_size),\n",
    "        \"type_token_ratio\": float(vocab_size / num_words) if num_words else 0.0,\n",
    "        \"hapax_ratio\": float(sum(1 for w, c in counts.items() if c == 1) / num_words) if num_words else 0.0,\n",
    "        \"uppercase_ratio\": float(sum(1 for c in text if c.isupper()) / max(num_chars, 1)),\n",
    "        \"punct_ratio\": float(len(punct) / max(num_chars, 1)),\n",
    "        \"comma_ratio\": float(text.count(\",\") / max(num_chars, 1)),\n",
    "        \"period_ratio\": float(text.count(\".\") / max(num_chars, 1)),\n",
    "        \"exclamation_ratio\": float(text.count(\"!\") / max(num_chars, 1)),\n",
    "        \"question_ratio\": float(text.count(\"?\") / max(num_chars, 1)),\n",
    "        \"digit_ratio\": float(sum(1 for c in text if c.isdigit()) / max(num_chars, 1)),\n",
    "        \"whitespace_ratio\": float(text.count(\" \") / max(num_chars, 1)),\n",
    "    }\n",
    "\n",
    "def extract_function_word_features(text):\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    num_words = len(words) if words else 1\n",
    "    counts = Counter(words)\n",
    "    func_count = sum(counts[w] for w in FUNCTION_WORDS if w in counts)\n",
    "    return {\n",
    "        \"function_word_ratio\": float(func_count / num_words),\n",
    "        \"function_word_count\": float(func_count)\n",
    "    }\n",
    "\n",
    "def extract_structure_features(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    num_lines = len(lines)\n",
    "    avg_line_len = np.mean([len(line) for line in lines]) if lines else 0.0\n",
    "    return {\n",
    "        \"num_lines\": float(num_lines),\n",
    "        \"avg_line_len\": float(avg_line_len)\n",
    "    }\n",
    "\n",
    "def extract_statistical_features(text):\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    if not words:\n",
    "        return {\"word_len_variance\": 0.0, \"word_len_skew\": 0.0}\n",
    "    lengths = [len(w) for w in words]\n",
    "    variance = float(np.var(lengths))\n",
    "    mean = np.mean(lengths)\n",
    "    skew = float(np.mean([(l - mean)**3 for l in lengths]) / (variance**1.5)) if variance > 0 else 0.0\n",
    "    return {\n",
    "        \"word_len_variance\": variance,\n",
    "        \"word_len_skew\": skew\n",
    "    }\n",
    "\n",
    "def extract_preposition_and_stopword_features(words):\n",
    "    num_words = len(words) if words else 1\n",
    "    prep_count = sum(1 for w in words if w in PREPOSITIONS)\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_count = sum(1 for w in words if w in stop_words)\n",
    "    except:\n",
    "        stop_count = 0\n",
    "    return {\n",
    "        \"preposition_ratio\": float(prep_count / num_words),\n",
    "        \"stopword_ratio\": float(stop_count / num_words)\n",
    "    }\n",
    "\n",
    "def extract_sentiment_features(text):\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "    except:\n",
    "        polarity = 0.0\n",
    "        subjectivity = 0.0\n",
    "    return {\n",
    "        \"sentiment_polarity\": float(polarity),\n",
    "        \"sentiment_subjectivity\": float(subjectivity)\n",
    "    }\n",
    "\n",
    "def extract_political_leaning_features(words):\n",
    "    left_count = sum(1 for w in words if w in LEFT_WORDS)\n",
    "    right_count = sum(1 for w in words if w in RIGHT_WORDS)\n",
    "    num_words = len(words) if words else 1\n",
    "    return {\n",
    "        \"left_word_ratio\": float(left_count / num_words),\n",
    "        \"right_word_ratio\": float(right_count / num_words)\n",
    "    }\n",
    "\n",
    "def extract_word_distribution_features(words):\n",
    "    if not words:\n",
    "        return {\"word_entropy\": 0.0}\n",
    "    counts = Counter(words)\n",
    "    total = sum(counts.values())\n",
    "    probs = [c / total for c in counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "    return {\"word_entropy\": float(entropy)}\n",
    "\n",
    "def extract_english_type_features(words):\n",
    "    us_count = sum(1 for w in words if w in US_SPELLINGS)\n",
    "    uk_count = sum(1 for w in words if w in UK_SPELLINGS)\n",
    "    num_words = len(words) if words else 1\n",
    "    return {\n",
    "        \"us_spelling_ratio\": float(us_count / num_words),\n",
    "        \"uk_spelling_ratio\": float(uk_count / num_words)\n",
    "    }\n",
    "\n",
    "def extract_agreement_score_features(text):\n",
    "    agreement_phrases = [\n",
    "        \"i agree\", \"i disagree\", \"absolutely\", \"definitely\",\n",
    "        \"certainly\", \"perhaps\", \"maybe\", \"probably\"\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    count = sum(text_lower.count(phrase) for phrase in agreement_phrases)\n",
    "    return {\"agreement_phrase_count\": float(count)}\n",
    "\n",
    "def extract_repeating_word_features(words):\n",
    "    if len(words) < 2:\n",
    "        return {\"repeated_word_ratio\": 0.0}\n",
    "    repeated = sum(1 for i in range(len(words)-1) if words[i] == words[i+1])\n",
    "    return {\"repeated_word_ratio\": float(repeated / len(words))}\n",
    "\n",
    "def extract_complexity_features(text):\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    sentences = [s for s in re.split(r\"[.!?]+\", text) if s.strip()]\n",
    "\n",
    "    if not words or not sentences:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": 0.0,\n",
    "            \"flesch_kincaid_grade\": 0.0,\n",
    "            \"avg_syllables_per_word\": 0.0\n",
    "        }\n",
    "\n",
    "    total_syllables = sum(count_syllables(w) for w in words)\n",
    "    avg_syllables = total_syllables / len(words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    flesch_reading_ease = 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables\n",
    "    flesch_kincaid_grade = 0.39 * avg_words_per_sentence + 11.8 * avg_syllables - 15.59\n",
    "\n",
    "    return {\n",
    "        \"flesch_reading_ease\": float(flesch_reading_ease),\n",
    "        \"flesch_kincaid_grade\": float(flesch_kincaid_grade),\n",
    "        \"avg_syllables_per_word\": float(avg_syllables)\n",
    "    }\n",
    "\n",
    "def extract_all_features(text):\n",
    "    feats = {}\n",
    "    feats.update(extract_lexical_features(text))\n",
    "    feats.update(extract_function_word_features(text))\n",
    "    feats.update(extract_structure_features(text))\n",
    "    feats.update(extract_statistical_features(text))\n",
    "\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    feats.update(extract_preposition_and_stopword_features(words))\n",
    "    feats.update(extract_sentiment_features(text))\n",
    "    feats.update(extract_political_leaning_features(words))\n",
    "    feats.update(extract_word_distribution_features(words))\n",
    "    feats.update(extract_english_type_features(words))\n",
    "    feats.update(extract_agreement_score_features(text))\n",
    "    feats.update(extract_repeating_word_features(words))\n",
    "    feats.update(extract_complexity_features(text))\n",
    "\n",
    "    return feats\n",
    "\n",
    "def build_feature_matrix(texts):\n",
    "    print(f\"Extracting features from {len(texts)} texts...\")\n",
    "    feature_dicts = [extract_all_features(t) for t in texts]\n",
    "    X = pd.DataFrame(feature_dicts)\n",
    "    X = X.fillna(0.0)\n",
    "    return X\n",
    "\n",
    "print(\"Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MijmnhMU8Z4L"
   },
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19301,
     "status": "ok",
     "timestamp": 1765919000899,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "data_loading",
    "outputId": "e57af66b-7761-4fbc-86e1-4137b1a18985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /content/drive/MyDrive/train_none.csv\n",
      "Successfully loaded 467985 rows\n",
      "Columns: ['id', 'adv_source_id', 'source_id', 'model', 'decoding', 'repetition_penalty', 'attack', 'domain', 'title', 'prompt', 'generation']\n",
      "\n",
      "DATASET SIZE INFO\n",
      "Full dataset size: 467985\n",
      "Train size BEFORE filtering: 374388\n",
      "Test size BEFORE filtering: 93597\n",
      "\n",
      "Using models: ['cohere-chat', 'gpt4', 'mistral-chat', 'mpt-chat', 'llama-chat']\n",
      "Train size AFTER filtering: 171149\n",
      "Test size AFTER filtering: 42787\n",
      "\n",
      "TRAIN label distribution:\n",
      "model\n",
      "mpt-chat        42787\n",
      "mistral-chat    42787\n",
      "llama-chat      42787\n",
      "gpt4            21394\n",
      "cohere-chat     21394\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST label distribution:\n",
      "model\n",
      "mistral-chat    10697\n",
      "mpt-chat        10697\n",
      "llama-chat      10697\n",
      "cohere-chat      5348\n",
      "gpt4             5348\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the RAID dataset\"\"\"\n",
    "    print(f\"Loading dataset from: {TRAIN_PATH}\")\n",
    "\n",
    "    # Try reading with different parameters\n",
    "    try:\n",
    "        full_df = pd.read_csv(TRAIN_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading with default settings: {e}\")\n",
    "        print(\"Trying alternative read methods...\")\n",
    "        try:\n",
    "            # Try without assuming it's CSV\n",
    "            full_df = pd.read_csv(TRAIN_PATH, sep=',', encoding='utf-8')\n",
    "        except:\n",
    "            # Try tab-separated\n",
    "            full_df = pd.read_csv(TRAIN_PATH, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    print(f\"Successfully loaded {len(full_df)} rows\")\n",
    "    print(f\"Columns: {list(full_df.columns)}\")\n",
    "\n",
    "    if \"generation\" not in full_df.columns or \"model\" not in full_df.columns:\n",
    "        raise ValueError(\"Expected columns: generation, model. Check the CSV file.\")\n",
    "\n",
    "    print(\"\\nDATASET SIZE INFO\")\n",
    "    print(f\"Full dataset size: {len(full_df)}\")\n",
    "\n",
    "    # Split into train/test\n",
    "    train_df, test_df = train_test_split(\n",
    "        full_df,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=full_df[\"model\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Train size BEFORE filtering: {len(train_df)}\")\n",
    "    print(f\"Test size BEFORE filtering: {len(test_df)}\")\n",
    "\n",
    "    # Filter to specific models if specified\n",
    "    if MODELS is not None:\n",
    "        train_df = train_df[train_df[\"model\"].isin(MODELS)]\n",
    "        test_df = test_df[test_df[\"model\"].isin(MODELS)]\n",
    "        print(f\"\\nUsing models: {MODELS}\")\n",
    "\n",
    "    print(f\"Train size AFTER filtering: {len(train_df)}\")\n",
    "    print(f\"Test size AFTER filtering: {len(test_df)}\")\n",
    "\n",
    "    train_texts = train_df[\"generation\"].astype(str)\n",
    "    train_labels = train_df[\"model\"]\n",
    "\n",
    "    test_texts = test_df[\"generation\"].astype(str)\n",
    "    test_labels = test_df[\"model\"]\n",
    "\n",
    "    print(\"\\nTRAIN label distribution:\")\n",
    "    print(train_labels.value_counts())\n",
    "\n",
    "    print(\"\\nTEST label distribution:\")\n",
    "    print(test_labels.value_counts())\n",
    "\n",
    "    return train_df, test_df, train_texts, test_texts, train_labels, test_labels\n",
    "\n",
    "# Load the data\n",
    "train_df, test_df, train_texts, test_texts, train_labels, test_labels = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnkYfp-g8Z4O"
   },
   "source": [
    "## Feature Extraction and CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 767086,
     "status": "ok",
     "timestamp": 1765919767988,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "feature_extraction",
    "outputId": "f0b9c6be-ca4b-4ee3-f1b4-d04beff96f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Extracting features from 171149 texts...\n",
      "Extracting features from 42787 texts...\n",
      "Saving features to extracted_features.csv\n",
      "Feature count: 41\n",
      "Train features shape: (171149, 41)\n",
      "Test features shape: (42787, 41)\n"
     ]
    }
   ],
   "source": [
    "def extract_and_save_features(train_texts, test_texts, train_labels, test_labels):\n",
    "    \"\"\"Extract features and save to CSV\"\"\"\n",
    "    features_path = FEATURES_CSV\n",
    "\n",
    "    if os.path.exists(features_path):\n",
    "        print(f\"Loading existing features from {features_path}\")\n",
    "        features_df = pd.read_csv(features_path)\n",
    "\n",
    "        # Split back into train/test\n",
    "        train_size = len(train_texts)\n",
    "        X_train_num = features_df.iloc[:train_size].drop(['label', 'split'], axis=1)\n",
    "        X_test_num = features_df.iloc[train_size:].drop(['label', 'split'], axis=1)\n",
    "    else:\n",
    "        print(\"Extracting features...\")\n",
    "        X_train_num = build_feature_matrix(train_texts)\n",
    "        X_test_num = build_feature_matrix(test_texts)\n",
    "\n",
    "        # Combine and save\n",
    "        print(f\"Saving features to {features_path}\")\n",
    "        X_train_num['label'] = train_labels.values\n",
    "        X_train_num['split'] = 'train'\n",
    "        X_test_num['label'] = test_labels.values\n",
    "        X_test_num['split'] = 'test'\n",
    "\n",
    "        features_df = pd.concat([X_train_num, X_test_num], axis=0)\n",
    "        features_df.to_csv(features_path, index=False)\n",
    "\n",
    "        # Remove temporary columns\n",
    "        X_train_num = X_train_num.drop(['label', 'split'], axis=1)\n",
    "        X_test_num = X_test_num.drop(['label', 'split'], axis=1)\n",
    "\n",
    "    print(f\"Feature count: {X_train_num.shape[1]}\")\n",
    "    print(f\"Train features shape: {X_train_num.shape}\")\n",
    "    print(f\"Test features shape: {X_test_num.shape}\")\n",
    "\n",
    "    return X_train_num, X_test_num\n",
    "\n",
    "# Extract features\n",
    "X_train_num, X_test_num = extract_and_save_features(\n",
    "    train_texts, test_texts, train_labels, test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj6KD-EO8Z4Q"
   },
   "source": [
    "## TF-IDF Feature Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272019,
     "status": "ok",
     "timestamp": 1765920040012,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "tfidf_features",
    "outputId": "2ad8cadb-da57-4082-dddc-e969e27f2002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF features...\n",
      "TF-IDF train shape: (171149, 5000)\n",
      "Combined train shape: (171149, 5041)\n"
     ]
    }
   ],
   "source": [
    "def build_tfidf_features(train_texts, test_texts, X_train_num, X_test_num):\n",
    "    \"\"\"Build and combine TF-IDF features with numerical features\"\"\"\n",
    "    tfidf_path = os.path.join(MODEL_DIR, \"tfidf_vectorizer.pkl\")\n",
    "    train_tfidf_path = os.path.join(MODEL_DIR, \"X_train_tfidf.npz\")\n",
    "    test_tfidf_path = os.path.join(MODEL_DIR, \"X_test_tfidf.npz\")\n",
    "\n",
    "    if os.path.exists(tfidf_path) and os.path.exists(train_tfidf_path):\n",
    "        print(\"Loading saved TF-IDF features...\")\n",
    "        tfidf = joblib.load(tfidf_path)\n",
    "        X_train_tfidf = load_npz(train_tfidf_path)\n",
    "        X_test_tfidf = load_npz(test_tfidf_path)\n",
    "    else:\n",
    "        print(\"Building TF-IDF features...\")\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.9\n",
    "        )\n",
    "        X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "        X_test_tfidf = tfidf.transform(test_texts)\n",
    "\n",
    "        # Save TF-IDF\n",
    "        joblib.dump(tfidf, tfidf_path)\n",
    "        save_npz(train_tfidf_path, X_train_tfidf)\n",
    "        save_npz(test_tfidf_path, X_test_tfidf)\n",
    "\n",
    "    # Convert numerical features to sparse\n",
    "    X_train_num_sp = csr_matrix(X_train_num.values)\n",
    "    X_test_num_sp = csr_matrix(X_test_num.values)\n",
    "\n",
    "    # Combine TF-IDF with numerical features\n",
    "    X_train_combined = hstack([X_train_tfidf, X_train_num_sp], format=\"csr\")\n",
    "    X_test_combined = hstack([X_test_tfidf, X_test_num_sp], format=\"csr\")\n",
    "\n",
    "    print(f\"TF-IDF train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"Combined train shape: {X_train_combined.shape}\")\n",
    "\n",
    "    return X_train_combined, X_test_combined, tfidf\n",
    "\n",
    "# Build TF-IDF features\n",
    "X_train, X_test, tfidf = build_tfidf_features(\n",
    "    train_texts, test_texts, X_train_num, X_test_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uibfyXpP8Z4S"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1765920040028,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "label_encoding",
    "outputId": "c59779d5-108f-4239-fad4-7e0d1f6b9d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes:\n",
      "['cohere-chat', 'gpt4', 'llama-chat', 'mistral-chat', 'mpt-chat']\n",
      "Number of classes: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/label_encoder.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "\n",
    "print(\"\\nClasses:\")\n",
    "print(list(le.classes_))\n",
    "print(f\"Number of classes: {len(le.classes_)}\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkXfJIHA8Z4U"
   },
   "source": [
    "## Traditional ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7820829,
     "status": "ok",
     "timestamp": 1765927860860,
     "user": {
      "displayName": "Aparajita Sarkar",
      "userId": "03701883905252281405"
     },
     "user_tz": 300
    },
    "id": "traditional_ml",
    "outputId": "4a9ba004-a4cb-48a3-e4ac-42a0e4daddc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RANDOM FOREST\n",
      "==================================================\n",
      "Training random_forest...\n",
      "Saved random_forest to saved_models/random_forest.pkl\n",
      "Accuracy: 0.7786\n",
      "Macro F1: 0.7768\n",
      "\n",
      "==================================================\n",
      "XGBOOST\n",
      "==================================================\n",
      "Training xgboost...\n",
      "Saved xgboost to saved_models/xgboost.pkl\n",
      "Accuracy: 0.8278\n",
      "Macro F1: 0.8300\n",
      "\n",
      "==================================================\n",
      "LIGHTGBM\n",
      "==================================================\n",
      "Training lightgbm...\n",
      "Saved lightgbm to saved_models/lightgbm.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8618\n",
      "Macro F1: 0.8656\n",
      "\n",
      "==================================================\n",
      "LOGISTIC REGRESSION\n",
      "==================================================\n",
      "Training logistic_regression...\n",
      "Saved logistic_regression to saved_models/logistic_regression.pkl\n",
      "Accuracy: 0.4920\n",
      "Macro F1: 0.4480\n"
     ]
    }
   ],
   "source": [
    "def train_or_load_model(model_name, model_class, model_params, X_train, y_train):\n",
    "    \"\"\"Train a model or load from disk if it exists\"\"\"\n",
    "    model_path = os.path.join(MODEL_DIR, f\"{model_name}.pkl\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading {model_name} from {model_path}\")\n",
    "        model = joblib.load(model_path)\n",
    "    else:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Saved {model_name} to {model_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Dictionary to store all models and predictions\n",
    "models = {}\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*50)\n",
    "models['RandomForest'] = train_or_load_model(\n",
    "    'random_forest',\n",
    "    RandomForestClassifier,\n",
    "    {'n_estimators': 200, 'random_state': RANDOM_STATE, 'n_jobs': -1},\n",
    "    X_train, y_train\n",
    ")\n",
    "predictions['RandomForest'] = models['RandomForest'].predict(X_test)\n",
    "probabilities['RandomForest'] = models['RandomForest'].predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions['RandomForest']):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, predictions['RandomForest'], average='macro'):.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"XGBOOST\")\n",
    "print(\"=\"*50)\n",
    "models['XGBoost'] = train_or_load_model(\n",
    "    'xgboost',\n",
    "    XGBClassifier,\n",
    "    {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(le.classes_),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': RANDOM_STATE\n",
    "    },\n",
    "    X_train, y_train\n",
    ")\n",
    "predictions['XGBoost'] = models['XGBoost'].predict(X_test)\n",
    "probabilities['XGBoost'] = models['XGBoost'].predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions['XGBoost']):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, predictions['XGBoost'], average='macro'):.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LIGHTGBM\")\n",
    "print(\"=\"*50)\n",
    "models['LightGBM'] = train_or_load_model(\n",
    "    'lightgbm',\n",
    "    lgb.LGBMClassifier,\n",
    "    {\n",
    "        'n_estimators': 300,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    },\n",
    "    X_train, y_train\n",
    ")\n",
    "predictions['LightGBM'] = models['LightGBM'].predict(X_test)\n",
    "probabilities['LightGBM'] = models['LightGBM'].predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions['LightGBM']):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, predictions['LightGBM'], average='macro'):.4f}\")\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*50)\n",
    "models['LogisticRegression'] = train_or_load_model(\n",
    "    'logistic_regression',\n",
    "    LogisticRegression,\n",
    "    {\n",
    "        'max_iter': 1000,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "    X_train, y_train\n",
    ")\n",
    "predictions['LogisticRegression'] = models['LogisticRegression'].predict(X_test)\n",
    "probabilities['LogisticRegression'] = models['LogisticRegression'].predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions['LogisticRegression']):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(y_test, predictions['LogisticRegression'], average='macro'):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
